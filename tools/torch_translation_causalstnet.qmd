---
title: STConvNet Workbench
author: Insang Song
date: today
output:
    quarto::html_document
---



```{python}
import numpy as np
import matplotlib.pyplot as plt
# import torch
import geopandas as gpd
import geoplot
import polars as pl

aq = pl.read_csv("./data/microsoft_urban_air_data/airquality.csv", \
                 null_values = "NULL", try_parse_dates = True, infer_schema_length = 100_000)
city = pl.read_csv("./data/microsoft_urban_air_data/city.csv", \
                 null_values = "NULL", try_parse_dates = True, infer_schema_length = 100_000)
distr = pl.read_csv("./data/microsoft_urban_air_data/district.csv", \
                 null_values = "NULL", try_parse_dates = True, infer_schema_length = 100_000)
met = pl.read_csv("./data/microsoft_urban_air_data/meteorology.csv", \
                 null_values = "NULL", try_parse_dates = True, infer_schema_length = 100_000)
station = pl.read_csv("./data/microsoft_urban_air_data/station.csv", \
                 null_values = "NULL", try_parse_dates = True, infer_schema_length = 100_000)
fore = pl.read_csv("./data/microsoft_urban_air_data/weatherforecast.csv", \
                 null_values = "NULL", try_parse_dates = True, infer_schema_length = 1_000_000)
```

```{python}
import shapely as sply
city_gdf = gpd.GeoDataFrame(city, \
    geometry = sply.points(coords = city[["longitude", "latitude"]]), \
        crs = "EPSG:4326")
city_gdf


fig, ax = plt.subplots(figsize = (8, 8))
geoplot.pointplot(city_gdf, figsize = (5, 5), ax = ax)
fig.show()

station_gdf = gpd.GeoDataFrame(station, \
    geometry = sply.points(coords = station[["longitude", "latitude"]]), \
    crs = "EPSG:4326")

fig, ax = plt.subplots(figsize = (8, 8))
geoplot.pointplot(station_gdf, ax = ax)
fig.show()


```


```{r}
library(sf)
library(dplyr)
library(tidyr)
library(torch)
sf_use_s2(F)
```

```{r}
library(torch)
library(coro)
library(luz)



SimpleRNN <- nn_module(
  classname = "SimpleRNN",
  
  initialize = function(input_size, hidden_size, output_size = 1, num_layers = 1, dropout = 0.25) {
    self$hidden_size <- hidden_size
    self$rnn <- nn_rnn(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        nonlinearity = "relu",
        dropout = dropout,
        batch_first = TRUE
    )
    self$linear <- nn_linear(hidden_size, output_size)
  },
  
  forward = function(x) {
    output <- torch::nn_rnn(x)$output
    hidden <- torch::nn_rnn(x)$h_n
    pred <- torch::nn_linear(output[, -1, ])
    return(list(pred, hidden))
  },

  init_hidden = function() {
    return(torch::torch_randn(1, 24, self$hidden_size))
  }
)

SimpleRNN(24, 32, 1, 1, 0.25)


SimpleGRU <- nn_module(
  classname = "SimpleGRU",
  
  initialize = function(input_size, hidden_size, output_size = 1, num_layers = 1, dropout = 0.25) {
    self$hidden_size <- hidden_size
    self$gru <- nn_gru(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        dropout = dropout,
        batch_first = TRUE
    )
    self$linear <- nn_linear(hidden_size, output_size)
  },
  
  forward = function(x) {
    
    output <- torch::nn_gru(x)$output
    hidden <- torch::nn_gru(x)$h_n
    pred <- torch::nn_linear(output[, -1, ])
    return(list(pred, hidden))
  },

  init_hidden = function() {
    return(torch::torch_randn(1, 24, self$hidden_size))
  }
)

SimpleGRU(24, 32, 1, 1, 0.25)


SimpleLSTM <- nn_module(
  classname = "SimpleLSTM",
  
  initialize = function(input_size, hidden_size, output_size = 1, num_layers = 1, dropout = 0.25) {
    self$hidden_size <- hidden_size
    self$lstm <- nn_lstm(
        input_size = input_size,
        hidden_size = hidden_size,
        num_layers = num_layers,
        dropout = dropout,
        batch_first = TRUE
    )
    self$linear <- nn_linear(hidden_size, output_size)
  },
  
  forward = function(x) {
    h <- torch::zeros(x$size(1), self$lstm$hidden_size)
    c <- torch::zeros(x$size(1), self$lstm$hidden_size)
    out <- list()
    for (i in seq_len(x$size(2))) {
      out[[i]] <- self$lstm$forward(x[, i, ], h, c)
    }
    ## check above
    pred <- torch::nn_linear(output[, -1, ])
    return(pred)
    # out <- torch::stack(out, dim = 1)
    # out <- self$linear$forward(out)
    # return(out)
  },

  init_hidden = function() {
    return(torch::torch_randn(1, 24, self$hidden_size))
  }
)

SimpleLSTM(24, 32, 1, 1, 0.25)

TCN <- nn_module(
  classname = "TCN",
  
  initialize = function(input_size, output_size, num_channels, kernel_size, dropout = 0.2) {
    
    self$tcn <- TemporalConvNet(input_size, num_channels, kernel_size, dropout = dropout)
    self$linear <- nn_linear(num_channels[length(num_channels)], output_size)
  },
  
  forward = function(x) {
    output <- torch::torch_transpose(self$tcn(torch::torch_transpose(x, 1, 2)), 1, 2)
    pred <- self$linear(output[ , -1, ])
    return(pred)
  }
)

TCN(32, 16, c(16, 48, 64, 256, 128, 32, 4), 2, 0.25)


Chomp1d <- nn_module(
    classname = "Chomp1d",

    initialize = function(chomp_size) {
        self$chomp_size <- chomp_size
    },

    forward = function(x) {
        x[, , 1:(x$size(3) - self$chomp_size)]
    }
)


TemporalBlock <- nn_module(
  classname = "TemporalBlock",
  
  initialize = function(n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout = 0.2) {
    
    self$conv1 <- nn_conv1d(n_inputs, n_outputs, kernel_size, stride = stride, padding = padding, dilation = dilation)
    self$chomp1 <- Chomp1d(padding)  # You need to implement this
    self$relu1 <- nn_relu()
    self$dropout1 <- nn_dropout(dropout)
    
    self$conv2 <- nn_conv1d(n_outputs, n_outputs, kernel_size, stride = stride, padding = padding, dilation = dilation)
    self$chomp2 <- Chomp1d(padding)  # You need to implement this
    self$relu2 <- nn_relu()
    self$dropout2 <- nn_dropout(dropout)
    
    self$net <- nn_module_list(
        list(
            self$conv1, self$chomp1, self$relu1, self$dropout1,
            self$conv2, self$chomp2, self$relu2, self$dropout2)
    )
    
    if (n_inputs != n_outputs) {
      self$downsample <- nn_conv1d(n_inputs, n_outputs, 1)
    } else {
      self$downsample <- NULL
    }
    
    self$relu <- nn_relu()
    self$init_weights()
  },
  
  init_weights = function() {
    # self$conv1$weight <- 
    torch::nn_init_normal_(self$conv1$weight, 0, 0.01)
    # self$conv2$weight <- 
    torch::nn_init_normal_(self$conv2$weight, 0, 0.01)
    if (!is.null(self$downsample)) {
      # self$downsample$weight <- 
      torch::nn_init_normal_(self$downsample$weight, 0, 0.01)
    }
  },
  
  forward = function(x) {
    out <- self$net$forward(x)
    res <- if (is.null(self$downsample)) x else self$downsample$forward(x)
    self$relu$forward(out + res)
  }
)

TemporalConvNet <- nn_module(
  classname = "TemporalConvNet",
  
  initialize = function(num_inputs, num_channels, kernel_size = 2, dropout = 0.2) {
    
    layers <- list()
    num_levels <- length(num_channels)
    for (i in seq_len(num_levels)) {
      dilation_size <- 2 ** (i - 1)
      in_channels <- if (i == 1) num_inputs else num_channels[[i - 1]]
      out_channels <- num_channels[[i]]
      layers <- c(layers, TemporalBlock$new(in_channels, out_channels, kernel_size, stride = 1, dilation = dilation_size,
                                            padding = (kernel_size - 1) * dilation_size, dropout = dropout))
    }
    
    # unlist(layers)?
    self$network <- nn_module_list(layers)
  },
  
  forward = function(x) {
    self$network$forward(x)
  }
)

TemporalBlock(64, 256, 5, 3, 2, 1, 0.25) %>%
torch::optim_adam(.$parameters, lr = 0.001)
TemporalConvNet(64, c(64, 128, 256, 512), 5, 0.25)



STCN <- torch::nn_module(
    initialize = function(input_size, in_channels, output_size, num_channels, kernel_size, dropout) {
        self$conv = torch::nn_sequential(
            torch::nn_conv2d(in_channels=in_channels, out_channels=64, kernel_size=c(1, 1), stride=1, padding=0),
            torch::nn_batch_norm2d(64),
            torch::nn_relu(),
            torch::nn_conv2d(in_channels=64, out_channels=1, kernel_size=c(1, 1), stride=1, padding=0),
            torch::nn_batch_norm2d(1),
            torch::nn_relu()
        )
        self$tcn = TemporalConvNet$new(input_size, num_channels, kernel_size, dropout=dropout)
        self$linear = torch::nn_linear(num_channels[-length(num_channels)], output_size)
    },

    forward = function(x) {
        conv_out <- torch::torch_squeeze(self$conv(x), 0)
        output <- torch::torch_transpose(self$tcn(torch::torch_transpose(conv_out, 1, 2)), 1, 2)
        pred <- self$linear(output[, -1, ])
        return(pred)
    }
)


```

```{python}
exvec = [1, 3, 4, 5]
# the first last
exvec[-1]

```

```{r torch-config}
rand_seed = 2024

model_name = 'STCN'  # ['RNN', 'GRU', 'LSTM', 'TCN', 'STCN']
device = 'cpu'  # 'cpu' or 'cuda'
input_size = 12
hidden_size = 32
output_size = 1
num_layers = 4
nlevels = 4
kernel_size = 4
dropout = 0.25
in_channels = 18

batch_size = 1
learning_rate = 1e-3
n_epochs = 50


```


```{r data_process_port}
pkgs <- c("data.table")
invisible(sapply(pkgs, library, character.only = TRUE, quietly = TRUE))


# extract station id list in Beijing
df_airq <- fread('./data/microsoft_urban_air_data/airquality.csv')
station_id_list <- unique(df_airq$station_id)[1:36]     # first 36 stations are in Beijing -- Why 36?
print(station_id_list)
    
# Calculate the influence degree (defined as the Pearson correlation coefficient) between the center station and other stations
r_thred <- 0.85
center_station_id <- 1013



#' This function generates tensors for training a spatio-temporal causal convolutional neural network.
#' @param center_station_id The ID of the center station.
#' @param station_id_list A list of station IDs.
#' @param r_thred The correlation threshold.
#' @note Assume that the data is stored in the `./data/stations_data` directory.
#' @returns A list containing the input tensor (x) and the target tensor (y).
make_tensors <- function(
    center_station_id = center_station_id,
    station_id_list = station_id_list,
    r_thred = r_thred
) {
    station_id_related_list <- c()
    df_one_station <- fread(paste0('./data/stations_data/df_station_', center_station_id, '.csv'))
    v_list_1 <- df_one_station$PM25_Concentration
    for (station_id_other in station_id_list) {
        df_one_station_other <- fread(paste0('./data/stations_data/df_station_', station_id_other, '.csv'))
        v_list_2 <- df_one_station_other$PM25_Concentration
        r <- cor(v_list_1, v_list_2)
        if (r > r_thred) {
            station_id_related_list <- c(station_id_related_list, station_id_other)
        }
        print(paste(center_station_id, station_id_other, round(r, 3)))
    }
    print(sprintf("%d %s", length(station_id_related_list), station_id_related_list))
    
    # generate x and y
    # x_shape: [example_count, num_releated, seq_step, feat_size]
    # y_shape: [example_count,]
    print(paste('Center station:', center_station_id))
    print(paste('Related stations:', station_id_related_list))
    feat_names <- c('PM25_Concentration', 'PM10_Concentration', 'NO2_Concentration', 'CO_Concentration', 'O3_Concentration', 'SO2_Concentration',
                  'weather', 'temperature', 'pressure', 'humidity', 'wind_speed', 'wind_direction')
    x_length <- 24
    y_length <- 1
    y_step <- 1
    x <- list()
    y <- list()
    for (station_id in station_id_related_list) {
        df_one_station <- fread(paste0('./data/stations_data/df_station_', station_id, '.csv'))
        x_one <- list()
        for (start_id in seq(1, nrow(df_one_station) - x_length - y_length + 1 - y_step + 1, y_length)) {
            x_data <- as.matrix(df_one_station[start_id:(start_id + x_length - 1), feat_names, with = FALSE])
            y_list <- df_one_station[(start_id + x_length + y_step - 1):(start_id + x_length + y_length + y_step - 2), PM25_Concentration, with = TRUE]
            if (any(is.na(x_data)) || any(is.na(y_list))) {
                next
            }
            x_one <- c(x_one, torch_tensor(x_data))
            if (station_id == center_station_id) {
                y <- c(y, mean(y_list))
            }
        }
        x_one <- torch_stack(x_one)
        if (length(x_one) <= 0) {
            next
        }
        x <- c(x, x_one)
        print(paste('station_id: ', station_id, ' x_shape: (', paste(dim(x_one), collapse = ", "), ")", sep = ""))
    }

    xx <- torch::torch_stack(x, dim = 1)
    xx <- torch::torch_transpose(xx, 1, 2)
    y <- torch::torch_tensor(unlist(y))
    
    print(paste('x_shape:', dim(x), 'y_shape:', dim(y)))
    return(list(xtensor = xx, ytensor = y))
}

```


```{r utils}
# FILEPATH: /home/felix/GitHub/ST-CausalConvNet/allflow.qmd
# BEGIN: ed8c6549bwf9

get_ids_for_tvt <- function() {
    train_ids <- c()
    valid_ids <- c()
    test_ids <- c()
    days_in_months <- c(31, 30, 31, 31, 30, 31, 30, 31, 31, 28, 31, 30-1)  # May to April
    start_id <- 0
    for (i in seq_along(days_in_months)) {
        days <- days_in_months[i]
        split_id_0 <- start_id
        split_id_1 <- start_id + as.integer(days * 24 * 0.6)
        split_id_2 <- start_id + as.integer(days * 24 * 0.8)
        split_id_3 <- start_id + as.integer(days * 24)
        train_ids <- c(train_ids, seq(split_id_0, split_id_1, 1))
        valid_ids <- c(valid_ids, seq(split_id_1, split_id_2, 1))
        test_ids <- c(test_ids, seq(split_id_2, split_id_3, 1))
        start_id <- start_id + as.integer(days * 24)
    }
    return(list(train_ids = train_ids, valid_ids = valid_ids, test_ids = test_ids))
}


#' @param x A tensor of shape [example_count, num_releated, seq_step, feat_size].
#' @param y A tensor of shape [example_count,].
load_data <- function(x, y) {

    y <- array(y, dim = c(length(y), 1))
    if (length(dim(x)) == 3) {
        #ss <- preprocessing.StandardScaler()
        for (i in seq(dim(x)[3])) {
            x[, , i] <- scale(x[, , i])
        }
    }
    ids <- get_ids_for_tvt()
    x_train <- x[ids$train_ids, , ]
    y_train <- y[ids$train_ids, ]
    x_valid <- x[ids$valid_ids, , ]
    y_valid <- y[ids$valid_ids, ]
    x_test <- x[ids$test_ids, , ]
    y_test <- y[ids$test_ids, ]

    print(paste("x_shape:", dim(x), "y_shape:", dim(y)))
    print(paste("x_train_shape:", dim(x_train), "y_train_shape:", dim(y_train), "x_valid_shape:", dim(x_valid), "y_valid_shape:", dim(y_valid), "x_test_shape:", dim(x_test), "y_test_shape:", dim(y_test)))
    return(list(x_train = x_train, y_train = y_train, x_valid = x_valid, y_valid = y_valid, x_test = x_test, y_test = y_test))
}

xtss <- load_data(as.array(xx[, 13L, , ]), as.array(y))

get_param_number <- function(net) {
    total_num <- sum(sapply(net$parameters(), function(p) length(p)))
    trainable_num <- sum(sapply(net$parameters(), function(p) length(p[p$requires_grad])))
    return(list(total_num = total_num, trainable_num = trainable_num))
}

# END: ed8c6549bwf9

```



```{r}

train <- function(
    net,
    x_train,
    y_train,
    x_valid,
    y_valid,
    x_test,
    y_test,
    learning_rate = learning_rate,
    plot = FALSE
) {
    rmse_train_list <- vector()
    rmse_valid_list <- vector()
    mae_valid_list <- vector()
    y_valid_pred_final <- vector()
    optimizer <- torch::optim_adam(net$parameters, lr = learning_rate)
    criterion <- torch::nn_mse_loss()
    h_state <- NULL

    for (epoch in 1:n_epochs) {
        rmse_train <- 0.0
        cnt <- 0
        for (start in 1:(length(x_train) - batch_size + 1)) {
            net$train()
            progress <- start / (length(x_train) - batch_size + 1)

            x_input <- torch::torch_tensor(x_train[start:(start + batch_size)], dtype=torch::torch_float32())
            y_true <- torch::tensor(y_train[start:(start + batch_size)], dtype=torch::torch_float32())

            if (model_name == 'RNN' || model_name == 'GRU') {
                y_pred <- net(x_input, h_state)
                h_state <- y_pred$h_state$data
            } else {
                y_pred <- net(x_input)
            }

            loss <- criterion(y_pred, y_true)
            optimizer$zero_grad()
            loss$backward()
            optimizer$step()

            mse_train_batch <- loss$data
            rmse_train_batch <- sqrt(mse_train_batch)
            rmse_train <- rmse_train + mse_train_batch
            if (start %% ((length(x_train) - batch_size) / 5) == 0) {
                print(paste('epoch:', epoch, ' progress:', progress * 100, '%  loss:', loss, '  rmse:', rmse_train_batch))
            }
            cnt <- cnt + 1
        }
        rmse_train <- sqrt(rmse_train / cnt)

        # validation
        net$eval()
        y_valid_pred_final <- vector()
        rmse_valid <- 0.0
        cnt <- 0
        for (start in 1:(length(x_valid) - batch_size + 1)) {
            x_input_valid <- torch::torch_tensor(x_valid[start:(start + batch_size)], dtype = torch::torch_float32())
            y_true_valid <- torch::torch_tensor(y_valid[start:(start + batch_size)], dtype = torch::torch_float32())
            if (model_name == 'RNN' || model_name == 'GRU') {
                y_valid_pred <- net(x_input_valid, h_state)
            } else {
                y_valid_pred <- net(x_input_valid)
            }
            y_valid_pred_final <- c(y_valid_pred_final, as.vector(y_valid_pred$data))
            loss_valid <- criterion(y_valid_pred, y_true_valid)$data
            mse_valid_batch <- as.vector(loss_valid)
            rmse_valid_batch <- sqrt(mse_valid_batch)
            rmse_valid <- rmse_valid + mse_valid_batch
            cnt <- cnt + 1
        }
        y_valid_pred_final <- array(y_valid_pred_final, dim = c(-1, 1))
        rmse_valid <- sqrt(rmse_valid / cnt)
        mae_valid <- mean(abs(y_valid - y_valid_pred_final))

        rmse_train_list <- c(rmse_train_list, rmse_train)
        rmse_valid_list <- c(rmse_valid_list, rmse_valid)
        mae_valid_list <- c(mae_valid_list, mae_valid)
        
        # save the best model
        # if (rmse_valid == min(rmse_valid_list)) {
        #     saveRDS(net$state_dict, model_save_pth)
        # }

        print(paste('\n>>> epoch:', epoch, '  RMSE_train:', rmse_train, '  RMSE_valid:', rmse_valid, ' MAE_valid:', mae_valid,
                    '\n    RMSE_valid_min:', min(rmse_valid_list), '  MAE_valid_min:', min(mae_valid_list), '\n'))
    }
}

train_main <- function(
    model_name = "STCN",
    fulldata = NULL,
    input_size = input_size,
    output_size = output_size,
    hidden_size = NULL,
    num_layers = num_layers,
    in_channels = NULL,
    nlevels = nlevels,
    kernel_size = kernel_size,
    dropout = dropout
) {
    # Hyper Parameters
    set.seed(rand_seed)
    torch::local_torch_manual_seed(rand_seed)

    # Load data
    # print('\nLoading data...\n')
    # x_train, y_train, x_valid, y_valid, x_test, y_test <- utils::load_data(f_x=f_x, f_y=f_y)

    # Generate model
    net <- NULL
    if (model_name == 'RNN') {
        net <- SimpleRNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)
    } else if (model_name == 'GRU') {
        net <- SimpleGRU(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)
    } else if (model_name == 'LSTM') {
        net <- SimpleLSTM(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)
    } else if (model_name == 'TCN') {
        net <- TCN(input_size=input_size, output_size=output_size, num_channels=rep(hidden_size, levels), kernel_size=kernel_size, dropout=dropout)
    } else if (model_name == 'STCN') {
        net <- STCN(input_size=input_size, in_channels=in_channels, output_size=output_size,
                            num_channels=rep(hidden_size, nlevels), kernel_size=kernel_size, dropout=dropout)
    }
    print(paste('\n------------ Model structure ------------\nmodel name:', model_name, '\n', #net,
     '\n-----------------------------------------\n'))

    # Training
    print('\nStart training...\n')
    train(
        net,
        fulldata$x_train, fulldata$y_train, fulldata$x_valid, fulldata$y_valid, fulldata$x_test, fulldata$y_test)
}


xtsstr <-
train_main(model_name = "GRU", fulldata = lapply(xtss, torch::torch_tensor), input_size = input_size, output_size = output_size, hidden_size = hidden_size, num_layers = num_layers, in_channels = in_channels, nlevels = nlevels, kernel_size = kernel_size, dropout = dropout)

```