% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/base_learner.R
\name{fit_base_learner}
\alias{fit_base_learner}
\title{Base learner: tune hyperparameters and retrieve the best model}
\usage{
fit_base_learner(
  learner = c("mlp", "xgb", "lgb", "elnet"),
  dt_full,
  r_subsample = 0.3,
  model = NULL,
  folds = NULL,
  cv_mode = c("spatiotemporal", "spatial", "temporal"),
  tune_mode = "grid",
  tune_bayes_iter = 10L,
  tune_grid_in = NULL,
  tune_grid_size = 10L,
  learn_rate = 0.1,
  yvar = "Arithmetic.Mean",
  xvar = seq(5, ncol(dt_sample)),
  vfold = 5L,
  nthreads = 8L,
  trim_resamples = FALSE,
  return_best = TRUE,
  args_generate_cv = NULL,
  ...
)
}
\arguments{
\item{learner}{character(1). The base learner to be used.
Default is "mlp". Available options are "mlp", "xgb", "lgb", "elnet".}

\item{dt_full}{The full data table to be used for prediction.}

\item{r_subsample}{numeric(1). The proportion of rows to be used.}

\item{model}{The parsnip model object. Preferably generated from
\code{switch_model}.}

\item{folds}{pre-generated rset object with minimal number of columns.
If NULL, \code{vfold} should be numeric to be used in \link[rsample:vfold_cv]{rsample::vfold_cv}.}

\item{tune_mode}{character(1). Hyperparameter tuning mode.
Default is "grid", "bayes" is acceptable.}

\item{tune_bayes_iter}{integer(1). The number of iterations for
Bayesian optimization. Default is 10. Only used when \code{tune_mode = "bayes"}.}

\item{tune_grid_in}{data.frame object that includes the grid for
hyperparameter tuning. \code{tune_grid_size} rows will be randomly picked
from this data.frame for grid search.}

\item{tune_grid_size}{integer(1). The number of grid size for hyperparameter
tuning. Default is 10. Only used when \code{tune_mode = "grid"}.}

\item{learn_rate}{The learning rate for the model. For branching purpose.
Default is 0.1.}

\item{yvar}{The target variable.}

\item{xvar}{The predictor variables.}

\item{vfold}{The number of folds for cross-validation.}

\item{nthreads}{integer(1). The number of threads to be used for
tuning. Default is 8L. \code{learner = "elnet"} will utilize the multiple
threads in \code{\link[future:multicore]{future::multicore()}} plan.}

\item{trim_resamples}{logical(1). Default is TRUE, which replaces the actual
data.frames in splits column of \code{tune_results} object with NA.}

\item{return_best}{logical(1). If TRUE, the best tuned model is returned.}

\item{...}{Additional arguments to be passed.}
}
\value{
The fitted workflow.
}
\description{
Multilayer perceptron model with different configurations of
hidden units, dropout, activation, and learning rate using brulee
and tidymodels. With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.
}
\details{
LightGBM model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid or Bayesian optimization search.
With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.

XGBoost model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid or Bayesian optimization search.
With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.

Elastic net model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid search or Bayesian optimization.
\itemize{
\item MLP: Hyperparameters \code{hidden_units}, \code{dropout}, \code{activation},
and \code{learn_rate} are tuned. \verb{With tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (56 combinations per \code{learn_rate} for mlp).
\item XGBoost: Hyperparameters \code{mtry}, \code{ntrees}, and \code{learn_rate} are
tuned. With \code{tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (30 combinations per \code{learn_rate}).
\item LightGBM: Hyperparameters \code{mtry}, \code{ntrees}, and \code{learn_rate} are
tuned. With \code{tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (30 combinations per \code{learn_rate}).
\item Elastic net: Hyperparameters \code{mixture} and \code{penalty} are tuned.
}

Tuning is performed based on random grid search (size = 10).
}
\note{
tune package should be 1.2.0 or higher.
brulee, xgboost, and lightgbm should be installed with GPU support.
Grid search is not activated in this function, regardless of other parts'
description.
}
\keyword{Baselearner}
