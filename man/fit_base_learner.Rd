% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/base_learner.R
\name{fit_base_learner}
\alias{fit_base_learner}
\title{Base learner: tune hyperparameters and retrieve the best model}
\usage{
fit_base_learner(
  rset = NULL,
  model = NULL,
  tune_grid_size = 10L,
  yvar = "Arithmetic.Mean",
  xvar = NULL,
  drop_vars = NULL,
  normalize = TRUE,
  metric = "rmse",
  ...
)
}
\arguments{
\item{rset}{A space/time CV set generated from beethoven}

\item{model}{The parsnip model object. Preferably generated from
\code{switch_model}.}

\item{tune_grid_size}{numeric(1), finetune grid size.}

\item{yvar}{The target variable.}

\item{xvar}{The predictor variables.}

\item{drop_vars}{character vector or numeric.
The variables to be dropped from the data.frame.}

\item{normalize}{logical(1). If \code{TRUE}, all numeric predictors are
normalized. Default is \code{FALSE}.}

\item{metric}{character(1). The metric to be used for selecting the best.
Must be one of "rmse", "rsq", "msd", "mae". Default = "rmse"}

\item{...}{Additional arguments to be passed.}
}
\value{
The fitted workflow.
}
\description{
Multilayer perceptron model with different configurations of
hidden units, dropout, activation, and learning rate using brulee
and tidymodels. With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.
}
\details{
LightGBM model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid or Bayesian optimization search.
With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.

XGBoost model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid or Bayesian optimization search.
With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.

Elastic net model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid search or Bayesian optimization.
\itemize{
\item MLP: Hyperparameters \code{hidden_units}, \code{dropout}, \code{activation},
and \code{learn_rate} are tuned. \verb{With tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (56 combinations per \code{learn_rate} for mlp).
\item XGBoost: Hyperparameters \code{mtry}, \code{ntrees}, and \code{learn_rate} are
tuned. With \code{tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (30 combinations per \code{learn_rate}).
\item LightGBM: Hyperparameters \code{mtry}, \code{ntrees}, and \code{learn_rate} are
tuned. With \code{tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (30 combinations per \code{learn_rate}).
\item Elastic net: Hyperparameters \code{mixture} and \code{penalty} are tuned.
}

Tuning is performed based on random grid search (size = 10).
}
\note{
tune package should be 1.2.0 or higher.
brulee, xgboost, and lightgbm should be installed with GPU support.
Grid search is not activated in this function, regardless of other parts'
description.
}
\keyword{Baselearner}
