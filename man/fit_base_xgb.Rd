% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pipeline_base_functions.R
\name{fit_base_xgb}
\alias{fit_base_xgb}
\title{Base learner: Extreme gradient boosting (XGBoost)}
\usage{
fit_base_xgb(
  dt_imputed,
  folds = NULL,
  r_subsample = 0.3,
  tune_mode = "grid",
  learn_rate = 0.1,
  yvar = "Arithmetic.Mean",
  xvar = seq(5, ncol(dt_imputed)),
  vfold = 5L,
  device = "cuda:0",
  ...
)
}
\arguments{
\item{dt_imputed}{The input data table to be used for fitting.}

\item{folds}{pre-generated rset object. If NULL, it should be
numeric to be used in \link[rsample:vfold_cv]{rsample::vfold_cv}.}

\item{r_subsample}{The proportion of rows to be sampled.}

\item{tune_mode}{character(1). Hyperparameter tuning mode.
Default is "grid", "bayes" is acceptable.}

\item{learn_rate}{The learning rate for the model. For branching purpose.
Default is 0.1.}

\item{yvar}{The target variable.}

\item{xvar}{The predictor variables.}

\item{vfold}{The number of folds for cross-validation.}

\item{device}{The device to be used for training.
Default is "cuda:0". Make sure that your system is equipped
with CUDA-enabled graphical processing units.}

\item{...}{Additional arguments to be passed.}
}
\value{
The fitted workflow.
}
\description{
XGBoost model is fitted at the defined rate (\code{r_subsample}) of
the input dataset by grid search.
With proper settings, users can utilize graphics
processing units (GPU) to speed up the training process.
}
\details{
Hyperparameters \code{mtry}, \code{ntrees}, and \code{learn_rate} are
tuned. With \code{tune_mode = "grid"},
users can modify \code{learn_rate} explicitly, and other hyperparameters
will be predefined (30 combinations per \code{learn_rate}).
}
\note{
tune package should be 1.2.0 or higher.
xgboost should be installed with GPU support.
}
\keyword{Baselearner}
